# dashboard.py

import streamlit as st
import requests
import os
import difflib
import tempfile
from streamlit_ace import st_ace
from streamlit_webrtc import webrtc_streamer, WebRtcMode, ClientSettings
import numpy as np
import av
import streamlit.components.v1 as components
import wave
import json
import logging
import base64
import re
from datetime import datetime # Added import for datetime used in backend snippet


# === Logging Setup ===
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# === Constants ===
# Set the default backend URL based on your Railway deployment
DEFAULT_BACKEND_URL = "https://debugiq-backend.railway.app"
# Use environment variable BACKEND_URL if set, otherwise use the default
BACKEND_URL = os.getenv("BACKEND_URL", DEFAULT_BACKEND_URL)

# Define backend API endpoint URLs based on main.py routing
# Structure: f"{BACKEND_URL}/[prefix_from_main.py]/[path_in_router_file]"
# !!! IMPORTANT: Verify the exact paths within your router files (e.g., analyze.py, qa.py, etc.)
SUGGEST_PATCH_URL = f"{BACKEND_URL}/debugiq/suggest_patch" # Prefix /debugiq from main.py + assumed path /suggest_patch
QA_URL = f"{BACKEND_URL}/qa/run_qa" # Prefix /qa from main.py + assumed path /run_qa
DOC_URL = f"{BACKEND_URL}/doc/generate_doc" # Prefix /doc from main.py + assumed path /generate_doc
ISSUES_INBOX_URL = f"{BACKEND_URL}/issues_inbox" # No prefix + assumed path /issues_inbox
WORKFLOW_RUN_URL = f"{BACKEND_URL}/workflow/run" # Prefix /workflow from main.py + assumed path /run
WORKFLOW_CHECK_URL = f"{BACKEND_URL}/workflow/status" # Prefix /workflow from main.py + assumed path /status
METRICS_URL = f"{BACKEND_URL}/system_metrics" # No prefix + assumed path /system_metrics

# Voice/Chat related endpoints - verify paths in your voice/chat router files (e.g., voice_ws_router.py)
# Assuming these paths are defined directly without prefixes in the included voice router
COMMAND_URL = f"{BACKEND_URL}/command"
VOICE_TRANSCRIBE_URL = f"{BACKEND_URL}/transcribe_audio"
GEMINI_CHAT_URL = f"{BACKEND_URL}/gemini_chat"


DEFAULT_VOICE_SAMPLE_RATE = 16000
DEFAULT_VOICE_SAMPLE_WIDTH = 2  # 16-bit audio (2 bytes)
DEFAULT_VOICE_CHANNELS = 1  # Mono
AUDIO_PROCESSING_THRESHOLD_SECONDS = 2

TRACEBACK_EXTENSION = ".txt"
SUPPORTED_SOURCE_EXTENSIONS = (
    ".py", ".js", ".java", ".c", ".cpp", ".cs", ".go", ".rb", ".php",
    ".html", ".css", ".md", ".ts", ".tsx", ".json", ".yaml", ".yml",
    ".sh", ".R", ".swift", ".kt", ".scala"
)
RECOGNIZED_FILE_EXTENSIONS = SUPPORTED_SOURCE_EXTENSIONS + (TRACEBACK_EXTENSION,)

# === Helper Functions ===

def make_api_request(method, url, json_payload=None, files=None, operation_name="API Request"):
    """
    Placeholder function to make API requests to the backend.
    Replace with your actual implementation (e.g., using requests, handling auth, etc.).
    """
    logger.info(f"Attempting to make {method} request to {url} for {operation_name}")
    try:
        response = requests.request(method, url, json=json_payload, files=files, timeout=30)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
        logger.info(f"Successful {method} request to {url}")
        # Attempt to return JSON, handle cases where response might be empty or not JSON
        try:
            return response.json()
        except json.JSONDecodeError:
             logger.warning(f"Response for {url} was not JSON. Status: {response.status_code}")
             # Return a success indicator even if no JSON, if status code is 2xx
             if 200 <= response.status_code < 300:
                 return {"success": True, "message": "Request successful, no JSON response."}
             else:
                  return {"error": True, "details": f"Non-JSON response, status code: {response.status_code}"}

    except requests.exceptions.Timeout:
        st.error(f"‚è∞ {operation_name} failed: Request timed out. URL: {url}")
        logger.error(f"{operation_name} timed out: {url}")
        return {"error": True, "details": "Request timed out."}
    except requests.exceptions.RequestException as e:
        st.error(f"‚ùå {operation_name} failed: {e}. URL: {url}")
        logger.error(f"{operation_name} failed for {url}: {e}")
        return {"error": True, "details": str(e)}


def clear_github_selection_state():
    """Resets only GitHub-related selection state in the sidebar."""
    logger.info("Clearing GitHub selection state.")
    # Do NOT clear github_repo_url_input here, let the text_input widget manage its value
    st.session_state.current_github_repo_url = None
    st.session_state.github_branches = []
    st.session_state.github_selected_branch = None
    st.session_state.github_path_stack = [""]
    st.session_state.github_repo_owner = None
    st.session_state.github_repo_name = None
    # Note: This does NOT clear analysis results

def clear_analysis_inputs():
    """Clears loaded traceback and source files used as analysis inputs."""
    logger.info("Clearing analysis inputs (trace and sources).")
    st.session_state.analysis_results['trace'] = None
    st.session_state.analysis_results['source_files_content'] = {}

def clear_analysis_outputs():
     """Clears generated patch, explanation, and other analysis outputs."""
     logger.info("Clearing analysis outputs (patch, explanation, etc.).")
     st.session_state.analysis_results.update({
         'patch': None,
         'explanation': None,
         'doc_summary': None, # Assuming doc_summary comes from analysis results
         'patched_file_name': None,
         'original_patched_file_content': None
     })
     st.session_state.edited_patch = "" # Also clear any edited patch state

# === Session State Initialization ===
session_defaults = {
    "audio_sample_rate": DEFAULT_VOICE_SAMPLE_RATE,
    "audio_sample_width": DEFAULT_VOICE_SAMPLE_WIDTH,
    "audio_num_channels": DEFAULT_VOICE_CHANNELS,
    "audio_buffer": b"",
    "audio_frame_count": 0,
    "chat_history": [],
    "edited_patch": "",
    "github_repo_url_input": "", # Stores the value of the text input widget
    "current_github_repo_url": None, # Stores the URL that branches were successfully loaded for
    "github_branches": [],
    "github_selected_branch": None,
    "github_path_stack": [""], # Stack to manage current directory path in GitHub browser
    "github_repo_owner": None,
    "github_repo_name": None,
    "analysis_results": {
        "trace": None, # Loaded traceback content
        "source_files_content": {}, # Dictionary of {file_path: content} for source files
        "patch": None, # Generated or edited patch content
        "explanation": None, # Explanation of the patch
        "doc_summary": None, # Documentation summary (can come from analysis or separate doc gen)
        "patched_file_name": None, # Name of the file the patch applies to
        "original_patched_file_content": None # Original content of the file being patched
    },
    "qa_result": None, # Result of QA validation
    "inbox_data": None, # Data from the issues inbox
    "workflow_status_data": None, # Data from workflow status check
    "metrics_data": None, # Data from system metrics
    "qa_code_to_validate": None # Code specifically loaded for QA validation (maybe redundant with analysis_results['patch'])
}
for key, default_value in session_defaults.items():
    if key not in st.session_state:
        st.session_state[key] = default_value

# === Streamlit Page Configuration ===
st.set_page_config(page_title="DebugIQ Dashboard", layout="wide")
st.title("üß† DebugIQ Autonomous Debugging Dashboard")

# === GitHub Repo Integration Sidebar ===
with st.sidebar:
    st.markdown("### üì¶ Load Code from GitHub")

    # Text input for GitHub URL
    # The value is automatically managed by Streamlit in st.session_state.github_repo_url_input
    st.text_input(
        "Public GitHub Repository URL",
        placeholder="https://github.com/owner/repo",
        key="github_repo_url_input",
        # No on_change needed here to clear state, button click handles processing/clearing
    )

    # Button to trigger loading or resetting
    if st.button("Load/Process Repo", key="load_repo_button", use_container_width=True):
        # When the button is clicked, we explicitly trigger the load logic below
        # and handle clearing based on whether the input is empty or a new repo.
        input_url = st.session_state.get("github_repo_url_input", "").strip()
        if not input_url:
            # If the input is empty when the button is clicked, clear GitHub selection
            clear_github_selection_state()
            # Optionally clear analysis results if clearing the URL input means
            # the user wants to start fresh. Keeping analysis results might be confusing.
            clear_analysis_inputs()
            clear_analysis_outputs()
            st.info("GitHub input cleared and analysis reset.")
        else:
            # A valid URL is entered, mark for processing below
            # Setting current_github_repo_url to a value different from active_github_url
            # will trigger branch fetching logic below.
            # We also clear previous analysis results when explicitly loading a NEW repo.
            clear_analysis_inputs()
            clear_analysis_outputs()
            st.session_state.current_github_repo_url = "PROCESSING_" + input_url # Use a temp marker or None to trigger

        st.rerun() # Rerun to process the button click and state changes

    # --- GitHub URL Parsing & Branch Fetch ---
    # This logic runs on every rerun and is triggered when github_repo_url_input changes
    # and the Load/Process Repo button's logic sets current_github_repo_url to trigger it.
    active_github_url = st.session_state.get("github_repo_url_input", "").strip()
    current_loaded_url = st.session_state.get("current_github_repo_url")

    # Check if we need to fetch branches:
    # 1. There's an active URL in the input
    # 2. We haven't successfully loaded branches for this exact URL yet
    if active_github_url and current_loaded_url != active_github_url:
         match = re.match(r"https://github\.com/([^/]+)/([^/]+?)(?:\.git)?$", active_github_url)
         if not match:
             # If the URL format is invalid after the button is clicked, show error and reset GitHub state
             if current_loaded_url is not None: # Only show error if we attempted to process
                 st.warning("Invalid GitHub URL format. Use: https://github.com/owner/repo")
             clear_github_selection_state() # Clear branches, path, owner/repo names
             st.session_state.current_github_repo_url = None # Ensure it doesn't loop on invalid input
         else:
             owner, repo = match.groups()
             # Update owner/repo names
             st.session_state.github_repo_owner = owner
             st.session_state.github_repo_name = repo
             # Mark the URL as currently being processed
             st.session_state.current_github_repo_url = active_github_url

             api_branches_url = f"https://api.github.com/repos/{owner}/{repo}/branches"
             logger.info(f"Fetching branches: {api_branches_url}")
             try:
                 with st.spinner(f"Loading branches for {owner}/{repo}..."):
                     branches_res = requests.get(api_branches_url, timeout=10)
                     branches_res.raise_for_status()
                     st.session_state.github_branches = [b["name"] for b in branches_res.json()]
                 if st.session_state.github_branches:
                     # Set branch selection to the first branch if none selected or old selection invalid
                     if st.session_state.github_selected_branch not in st.session_state.github_branches:
                          st.session_state.github_selected_branch = st.session_state.github_branches[0]
                     st.session_state.github_path_stack = [""] # Reset path on successful repo load
                     st.success(f"Repo '{owner}/{repo}' branches loaded.")
                 else:
                     st.warning("No branches found for this repository.")
                     clear_github_selection_state() # Clear GitHub states if no branches
                     st.session_state.current_github_repo_url = None # Allow retry or indicate issue
             except requests.exceptions.RequestException as e:
                 st.error(f"‚ùå Branch fetch error: {e}")
                 clear_github_selection_state() # Clear GitHub states on error
                 st.session_state.current_github_repo_url = None # Allow retry
             except json.JSONDecodeError as e:
                  st.error(f"‚ùå Branch JSON error: Could not parse response. {e}")
                  clear_github_selection_state() # Clear GitHub states on error
                  st.session_state.current_github_repo_url = None # Allow retry


    # --- Branch Selection ---
    # Only show branch selector if branches were successfully loaded
    current_branches = st.session_state.get("github_branches", [])
    if current_branches:
        try:
            # Find the index of the currently selected branch, default to 0 if not found
            branch_idx = current_branches.index(st.session_state.github_selected_branch) if st.session_state.github_selected_branch in current_branches else 0
        except ValueError:
            # Fallback in case of unexpected value, should be covered by above check
            branch_idx = 0

        selected_branch = st.selectbox(
            "Select Branch",
            current_branches,
            index=branch_idx,
            key="github_branch_selector",
            on_change=lambda: st.session_state.update({"github_path_stack": [""]}) # Reset path on branch change
        )
        # Streamlit automatically updates st.session_state.github_selected_branch due to the key

    # --- File Browser Logic (if owner, repo, branch are set) ---
    gh_owner = st.session_state.get("github_repo_owner")
    gh_repo = st.session_state.get("github_repo_name")
    gh_branch = st.session_state.get("github_selected_branch")
    current_path_stack = st.session_state.get("github_path_stack", [""])
    current_path_str = "/".join([p for p in current_path_stack if p])


    if gh_owner and gh_repo and gh_branch:
        @st.cache_data(ttl=120, show_spinner="Fetching directory contents...")
        def fetch_gh_dir_content(owner_str, repo_str, path_str, branch_str):
            url = f"https://api.github.com/repos/{owner_str}/{repo_str}/contents/{path_str}?ref={branch_str}"
            logger.info(f"Fetching GitHub dir (cached): {url}")
            try:
                r = requests.get(url, timeout=10)
                r.raise_for_status()
                return r.json()
            except requests.exceptions.RequestException as e:
                st.warning(f"Dir content fetch error for '{path_str}': {e}")
            except json.JSONDecodeError as e:
                st.warning(f"Dir content JSON error for '{path_str}': {e}")
            return None

        entries = fetch_gh_dir_content(gh_owner, gh_repo, current_path_str, gh_branch)

        if entries is not None:
            # Display current path and "Up" button
            display_path = f"Current Path: /{current_path_str}" if current_path_str else "Current Path: / (Repo Root)"
            st.caption(display_path)
            if len(current_path_stack) > 1: # Not in root
                if st.button("‚¨ÜÔ∏è .. (Parent Directory)", key=f"gh_up_button_{current_path_str}", use_container_width=True):
                    st.session_state.github_path_stack.pop()
                    st.rerun()

            st.markdown("###### Directories")
            # Filter and sort directories
            dirs = sorted([item for item in entries if item["type"] == "dir"], key=lambda x: x["name"])
            for item in dirs:
                 if st.button(f"üìÅ {item['name']}", key=f"gh_dir_{current_path_str}_{item['name']}", use_container_width=True):
                     st.session_state.github_path_stack.append(item['name'])
                     st.rerun()

            st.markdown("###### Files")
            # Filter and sort recognized files
            files = sorted([item for item in entries if item["type"] == "file" and any(item["name"].endswith(ext) for ext in RECOGNIZED_FILE_EXTENSIONS)], key=lambda x: x["name"])
            for item in files:
                if st.button(f"üìÑ {item['name']}", key=f"gh_file_{current_path_str}_{item['name']}", use_container_width=True):
                    file_rel_path = f"{current_path_str}/{item['name']}".strip("/")
                    raw_url = f"https://raw.githubusercontent.com/{gh_owner}/{gh_repo}/{gh_branch}/{file_rel_path}"
                    logger.info(f"Fetching file: {raw_url}")
                    try:
                        with st.spinner(f"Loading {item['name']}..."):
                            file_r = requests.get(raw_url, timeout=10)
                            file_r.raise_for_status()
                        file_content = file_r.text

                        # Use relative path from repo root as key for analysis_results
                        key_path = os.path.join(*current_path_stack, item['name']).replace("\\","/") if current_path_str else item['name']

                        if item['name'].endswith(TRACEBACK_EXTENSION):
                            st.session_state.analysis_results['trace'] = file_content
                            st.info(f"'{key_path}' loaded as Traceback.")
                            # When loading a new traceback, clear previous analysis outputs
                            clear_analysis_outputs()
                        else: # Already filtered by RECOGNIZED_FILE_EXTENSIONS
                            st.session_state.analysis_results['source_files_content'][key_path] = file_content
                            st.info(f"'{key_path}' loaded as Source File.")

                        st.rerun() # To update "Loaded Analysis Inputs" expander and potentially other tabs
                    except requests.exceptions.RequestException as e:
                        st.error(f"Error loading file '{item['name']}': {e}")
            if not dirs and not files:
                 st.caption("No directories or recognized files found in this path.")

        elif current_path_str: # entries is None and not in root, indicating a fetch error
             st.warning("Could not list directory contents.")

    st.markdown("---")
    with st.expander("üì¨ Loaded Analysis Inputs", expanded=True):
        trace_val = st.session_state.analysis_results.get('trace')
        sources_val = st.session_state.analysis_results.get('source_files_content', {})
        if trace_val: st.text_area("Current Traceback:", value=trace_val, height=100, disabled=True, key="sidebar_trace_loaded_final")
        else: st.caption("No traceback loaded.")
        if sources_val:
            st.write("Current Source Files:")
            for name in sources_val.keys(): st.caption(f"- {name}")
        else: st.caption("No source files loaded.")
    st.markdown("---")

# === Manual File Uploader (Main Page Area) ===
st.markdown("### ‚¨ÜÔ∏è Or, Upload Files Manually")
manual_uploaded_files_main = st.file_uploader(
    "üìÑ Upload traceback (.txt) and/or source files",
    type=[ext.lstrip('.') for ext in RECOGNIZED_FILE_EXTENSIONS],
    accept_multiple_files=True,
    key="manual_uploader_main_page"
)

if manual_uploaded_files_main:
    manual_trace_str = None
    manual_sources_dict = {}
    manual_summary_list_main = []

    # Process uploaded files
    for file_item in manual_uploaded_files_main:
        try:
            # Read file content
            # file_item.getvalue() returns bytes, decode using utf-8
            content_bytes = file_item.getvalue()
            content_str = content_bytes.decode("utf-8", errors='ignore') # Use errors='ignore' for robustness

            if file_item.name.endswith(TRACEBACK_EXTENSION):
                manual_trace_str = content_str
                manual_summary_list_main.append(f"Traceback: {file_item.name}")
            elif any(file_item.name.endswith(ext) for ext in SUPPORTED_SOURCE_EXTENSIONS):
                manual_sources_dict[file_item.name] = content_str # Use file name as key
                manual_summary_list_main.append(f"Source: {file_item.name}")
            # Ignore files with unrecognized extensions
        except Exception as e:
            st.error(f"Error processing '{file_item.name}': {e}")
            logger.error(f"Error processing uploaded file {file_item.name}: {e}")


    # Decide how to update state based on what was uploaded
    # If files were uploaded, clear the GitHub selection state
    clear_github_selection_state()

    if manual_trace_str is not None:
        # If a traceback is uploaded, clear all previous analysis data (from GitHub or prior uploads)
        # and set the new traceback and any source files uploaded alongside it.
        clear_analysis_inputs()
        clear_analysis_outputs()
        st.session_state.analysis_results['trace'] = manual_trace_str
        st.session_state.analysis_results['source_files_content'] = manual_sources_dict # Only sources from this upload
        st.success(f"Manually uploaded: {', '.join(manual_summary_list_main)}. GitHub selection cleared.")
        st.rerun() # Rerun to update UI with new analysis inputs
    elif manual_sources_dict:
         # If only source files are uploaded (and no traceback in this upload),
         # merge them into the existing source files state. Keep existing traceback.
         st.session_state.analysis_results['source_files_content'].update(manual_sources_dict)
         st.success(f"Manually added source files: {', '.join(manual_summary_list_main)}. GitHub selection cleared.")
         st.rerun() # Rerun to update UI with new source files
    # If manual_uploaded_files_main was not empty, but no recognized files were processed,
    # no state change occurs except for clearing GitHub selection.

# === Main Application Tabs ===
tab_titles_main_list = [
    "üìÑ Traceback + Patch", "‚úÖ QA Validation", "üìò Documentation",
    "üì£ Issue Notices", "ü§ñ Autonomous Workflow", "üîç Workflow Check",
    "üìä Repo Structure Insights",
    "üìà Metrics"
]
# Assign tabs to variables
tab_patch, tab_qa, tab_doc, tab_issues, tab_workflow_trigger, \
    tab_workflow_status, tab_repo_insights, tab_metrics = st.tabs(tab_titles_main_list)

# Content for each tab - this code runs on every rerun, but the content is only displayed
# for the active tab. State accessed within these blocks reflects the current session state.

with tab_patch:
    st.header("üìÑ Traceback & Patch Analysis")
    loaded_trace_tab1 = st.session_state.analysis_results.get('trace')
    loaded_sources_tab1 = st.session_state.analysis_results.get('source_files_content', {})
    patched_code_val_tab1 = st.session_state.analysis_results.get('patch')
    original_content_tab1 = st.session_state.analysis_results.get('original_patched_file_content')
    explanation_val_tab1 = st.session_state.analysis_results.get('explanation')

    st.markdown("### Analysis Inputs")
    if loaded_trace_tab1:
        st.text_area("Loaded Traceback:", value=loaded_trace_tab1, height=150, disabled=True)
    else:
        st.info("Load a traceback from GitHub or manually upload to enable analysis.")

    if loaded_sources_tab1:
        with st.expander("Loaded Source Files:", expanded=False):
             st.json(loaded_sources_tab1)
    else:
         st.info("Load source files from GitHub or manually upload to provide context for analysis.")


    if st.button("üî¨ Analyze Loaded Data & Suggest Patch", key="tab1_analyze_main_btn", use_container_width=True):
        # This button triggers analysis based on the CURRENT inputs in session state
        # It should populate the analysis OUTPUTS state.
        if not loaded_trace_tab1 and not loaded_sources_tab1:
            st.warning("Please load data (traceback and/or source files) first from the sidebar or uploader.")
        else:
            # Prepare payload from current session state inputs
            payload = {
                "trace": loaded_trace_tab1,
                "language": "python", # Assuming language is always python for this example, adjust if needed
                "source_files": loaded_sources_tab1
                }
            # Call your backend API for patch suggestion
            response = make_api_request("POST", SUGGEST_PATCH_URL, json_payload=payload, operation_name="Patch Suggestion")

            if response and not response.get("error"):
                 # Update only the analysis OUTPUTS in session state based on the API response
                 st.session_state.analysis_results.update({
                     'patch': response.get("patched_code", ""),
                     'explanation': response.get("explanation"),
                     'patched_file_name': response.get("patched_file_name"),
                     'original_patched_file_content': response.get("original_content")
                 })
                 # Initialize the edited patch state with the suggested patch
                 st.session_state.edited_patch = st.session_state.analysis_results['patch']
                 st.success("Patch suggestion received.")
                 st.rerun() # Rerun to update UI with results, diff, and editor
            else:
                 # If analysis fails, clear the previous outputs and show an error
                 clear_analysis_outputs()
                 st.error(f"Patch analysis failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")


    st.markdown("---")
    st.markdown("### Analysis Results")

    if patched_code_val_tab1 is not None:
        st.markdown("### üîç Diff View")
        # Check if original content is available and different from the patch
        if original_content_tab1 is not None and original_content_tab1 != patched_code_val_tab1:
            html_diff_gen_tab1 = difflib.HtmlDiff(wrapcolumn=70, tabsize=4)
            diff_html = html_diff_gen_tab1.make_table(
                original_content_tab1.splitlines(keepends=True),
                patched_code_val_tab1.splitlines(keepends=True),
                "Original",
                "Patched",
                context=True,
                numlines=3
            )
            components.html(diff_html, height=450, scrolling=True)
        elif original_content_tab1 is not None and original_content_tab1 == patched_code_val_tab1:
             st.info("No changes in the suggested patch compared to the original file content.")
        elif patched_code_val_tab1 is not None:
             st.info("Original file content not available to generate a diff.")


        st.markdown("### ‚úçÔ∏è Edit Patch (Live)")
        # Use st_ace for live editing, its value is automatically stored in st.session_state.edited_patch
        edited_code_val_tab1 = st_ace(
            value=st.session_state.edited_patch, # Use the edited state as the source of truth for the editor
            language="python", # Set language highlighting
            theme="monokai", # Set editor theme
            height=300, # Set editor height
            key="tab1_ace_editor_main" # Key to store the value in session state
        )

        # If the user changed the code in the editor, update the 'patch' in analysis_results
        # This comparison happens on every rerun
        if edited_code_val_tab1 != st.session_state.analysis_results.get('patch'):
             st.session_state.analysis_results['patch'] = edited_code_val_tab1
             st.session_state.edited_patch = edited_code_val_tab1 # Keep edited_patch in sync
             # No rerun needed here, the editor change itself triggers a rerun


        if explanation_val_tab1:
             st.markdown(f"**Explanation:** {explanation_val_tab1}")

    elif loaded_trace_tab1 or loaded_sources_tab1:
         st.info("Run analysis above to generate a patch.")
    else:
         st.info("Load data and run analysis to see results here.")


with tab_qa:
    st.header("‚úÖ QA Validation")
    # Get the code for QA, preferably the edited patch if available, otherwise the suggested patch
    code_for_qa_tab2 = st.session_state.get('edited_patch') or st.session_state.analysis_results.get('patch')
    patched_file_name_tab2 = st.session_state.analysis_results.get('patched_file_name')

    if code_for_qa_tab2 is not None:
        st.text_area("Code for QA:", value=code_for_qa_tab2, height=200, disabled=True, key="qa_code_display_tab2_main_val")

        if st.button("üõ°Ô∏è Run QA Validation", key="qa_run_validation_btn_tab2_val", use_container_width=True):
            if not code_for_qa_tab2:
                 st.warning("No code available to validate.")
            else:
                 payload = {"code": code_for_qa_tab2, "patched_file_name": patched_file_name_tab2}
                 # Call your backend API for QA validation
                 response = make_api_request("POST", QA_URL, json_payload=payload, operation_name="QA Validation")
                 if response and not response.get("error"):
                     st.session_state.qa_result = response # Store QA result in session state
                     st.success("QA Validation complete.")
                     st.rerun() # Rerun to display result
                 else:
                     st.session_state.qa_result = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
                     st.error(f"QA Validation failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")

        # Display the QA result if available in session state
        qa_result_val_tab2 = st.session_state.get("qa_result")
        if qa_result_val_tab2:
            st.markdown("### QA Result:")
            if qa_result_val_tab2.get("error"):
                 st.error(f"QA Error: {qa_result_val_tab2.get('details', 'Unknown error.')}")
            else:
                 st.json(qa_result_val_tab2) # Display the full JSON result
        elif st.session_state.get("qa_result") is not None: # Handle case where result was explicitly set to None
             st.info("No QA result available yet. Run validation.")


    else:
        st.warning("No patched code from the 'Traceback + Patch' tab to validate.")


with tab_doc:
    st.header("üìò Documentation")
    # Display documentation summary from analysis results if available
    doc_summary_main_val = st.session_state.analysis_results.get('explanation') # Often explanation includes documentation aspects
    if doc_summary_main_val:
        st.markdown(f"**Documentation Summary from Analysis:** {doc_summary_main_val}")
        st.markdown("---")

    st.subheader("Generate Documentation for Specific Code:")
    # Text area for ad-hoc documentation generation
    doc_code_input_tab3_main_val = st.text_area("Paste code here:", key="doc_code_input_area_tab3_main_val", height=200)

    if st.button("üìù Generate Ad-hoc Documentation", key="doc_generate_btn_tab3_main_val", use_container_width=True):
        if not doc_code_input_tab3_main_val:
            st.warning("Please paste code into the text area to generate documentation.")
        else:
            payload = {"code": doc_code_input_tab3_main_val}
            # Call your backend API for documentation generation
            response = make_api_request("POST", DOC_URL, json_payload=payload, operation_name="Ad-hoc Docs")

            if response and not response.get("error"):
                # Display the generated documentation
                generated_doc = response.get("doc", "No documentation generated.")
                st.markdown("### Generated Documentation:")
                st.markdown(generated_doc)
            else:
                st.error(f"Documentation generation failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")

with tab_issues:
    st.header("üì£ Issue Notices & Agent Summaries")
    if st.button("üîÑ Refresh Issue Notices", key="fetch_issues_tab_btn_val", use_container_width=True):
        # Call your backend API to fetch issues inbox data
        response = make_api_request("GET", ISSUES_INBOX_URL, operation_name="Fetch Issues")
        if response and not response.get("error"):
            st.session_state.inbox_data = response # Store inbox data in session state
            st.success("Issue notices refreshed.")
            st.rerun() # Rerun to display new data
        else:
            st.session_state.inbox_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
            st.error(f"Issue fetch failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")

    # Display current inbox data from session state
    current_inbox = st.session_state.get("inbox_data")
    if current_inbox and not current_inbox.get("error"):
        issues_list = current_inbox.get("issues", [])
        if issues_list:
            st.markdown("### Current Issues:")
            for i, issue_item in enumerate(issues_list):
                with st.expander(f"Issue {issue_item.get('id','N/A')} - {issue_item.get('classification','N/A')}"):
                    st.json(issue_item)
                    # Button to trigger workflow for a specific issue
                    if st.button(f"‚ñ∂Ô∏è Trigger Workflow for Issue {issue_item.get('id','N/A')}", key=f"trigger_issue_{issue_item.get('id',i)}"):
                        # Call your backend API to trigger workflow
                        payload = {"issue_id": issue_item.get('id')}
                        workflow_response = make_api_request("POST", WORKFLOW_RUN_URL, json_payload=payload, operation_name=f"Trigger Workflow {issue_item.get('id','N/A')}")
                        if workflow_response and not workflow_response.get("error"):
                            st.success(f"Workflow for Issue {issue_item.get('id','N/A')} triggered successfully.")
                             # Optionally clear workflow status to force refresh on next tab check
                            st.session_state.workflow_status_data = None
                            st.rerun() # Rerun to update status tab potentially
                        else:
                            st.error(f"Failed to trigger workflow for Issue {issue_item.get('id','N/A')}. {workflow_response.get('details', '') if workflow_response else 'No response'}")
        else:
            st.info("No issues found in the inbox.")
    elif current_inbox and current_inbox.get("error"):
         st.error(f"Failed to load issues: {current_inbox.get('details', 'Unknown error.')}")
    else:
        st.info("Click 'Refresh Issue Notices' to fetch data.")


with tab_workflow_trigger:
    st.header("ü§ñ Trigger Autonomous Workflow by ID")
    workflow_issue_id_input_val = st.text_input("Enter Issue ID to trigger workflow:", key="workflow_issue_id_trigger_input_val")
    if st.button("‚ñ∂Ô∏è Run Workflow by ID", key="run_workflow_by_id_btn_val", use_container_width=True):
        if not workflow_issue_id_input_val:
            st.warning("Please enter an Issue ID to trigger a workflow.")
        else:
            payload = {"issue_id": workflow_issue_id_input_val}
            # Call your backend API to trigger workflow by ID
            response = make_api_request("POST", WORKFLOW_RUN_URL, json_payload=payload, operation_name=f"Manual Workflow Trigger {workflow_issue_id_input_val}")
            if response and not response.get("error"):
                st.success(f"Workflow for Issue {workflow_issue_id_input_val} triggered successfully.")
                 # Optionally clear workflow status to force refresh on next tab check
                st.session_state.workflow_status_data = None
                st.rerun() # Rerun to update status tab potentially
            else:
                st.error(f"Failed to trigger workflow for Issue {workflow_issue_id_input_val}. {response.get('details', '') if response else 'No response'}")

with tab_workflow_status:
    st.header("üîç Workflow Status Check")
    if st.button("üîÑ Refresh Workflow Status", key="refresh_workflow_status_check_btn_val", use_container_width=True):
        # Call your backend API to fetch workflow status
        response = make_api_request("GET", WORKFLOW_CHECK_URL, operation_name="Workflow Status")
        if response and not response.get("error"):
            st.session_state.workflow_status_data = response # Store status data
            st.success("Workflow status refreshed.")
            st.rerun() # Rerun to display new data
        else:
             st.session_state.workflow_status_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
             st.error(f"Workflow status check failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")

    # Display current workflow status data from session state
    current_status = st.session_state.get("workflow_status_data")
    if current_status and not current_status.get("error"):
        st.markdown("### Current Workflow Status:")
        st.json(current_status) # Display the full JSON status
    elif current_status and current_status.get("error"):
         st.error(f"Failed to load workflow status: {current_status.get('details', 'Unknown error.')}")
    else:
        st.info("Click 'Refresh Workflow Status' to fetch data.")

with tab_repo_insights:
    st.header("üìä Repository Structure Insights")
    st.markdown("""
    This tab is intended to display a "digital chart" or visual representation of your loaded GitHub repository's content.

    To implement this, I need a bit more information:

    1.  **What kind of chart are you envisioning?**
        * A **Treemap** or **Sunburst chart** to show file/directory sizes or counts?
        * A **Bar/Pie chart** for language distribution or file type breakdown?
        * Something else?

    2.  **What specific data from the repository should this chart visualize?**
        * File sizes?
        * Number of files per directory?
        * Lines of code per language (requires more advanced analysis on the backend)?
        * Distribution of file extensions?

    Once you provide these details, we can work on fetching the necessary data (likely via the GitHub API within the sidebar logic or a dedicated backend call) and then use a library like **Plotly Express** or **Altair** to create and display the interactive chart here.

    For example, if you wanted a chart of languages used in the repository, the sidebar could fetch that data when a repository is loaded, store it in `st.session_state`, and this tab would then use it to render the chart.
    """)

    # Example placeholder for where chart might go if data was available
    gh_owner_insight = st.session_state.get("github_repo_owner")
    gh_repo_insight = st.session_state.get("github_repo_name")

    if gh_owner_insight and gh_repo_insight:
        st.info(f"Insights for repository: {gh_owner_insight}/{gh_repo_insight} would appear here once specified.")
        # @st.cache_data # Example of fetching language data (uncomment and implement)
        # def get_repo_languages(owner, repo):
        #      lang_url = f"https://api.github.com/repos/{owner}/{repo}/languages"
        #      try:
        #          r = requests.get(lang_url, timeout=5)
        #          r.raise_for_status()
        #          return r.json() # Returns dict like {"Python": 30203, "JavaScript": 1024}
        #      except Exception as e:
        #          logger.error(f"Failed to fetch language data for {owner}/{repo}: {e}")
        #          return None

        # languages = get_repo_languages(gh_owner_insight, gh_repo_insight)
        # if languages:
        #     st.write("Language Breakdown (Example - actual chart TBD):")
        #     st.json(languages) # Replace with actual chart later using Plotly, Altair, etc.
        # else:
        #      st.warning("Could not fetch language data for the repository to display an example chart.")
    else:
        st.warning("Load a GitHub repository from the sidebar to see potential insights.")


with tab_metrics:
    st.header("üìà System Metrics")
    if st.button("üìà Fetch System Metrics", key="fetch_metrics_tab_btn_val", use_container_width=True):
        # Call your backend API to fetch system metrics
        response = make_api_request("GET", METRICS_URL, operation_name="Fetch Metrics")
        if response and not response.get("error"):
            st.session_state.metrics_data = response # Store metrics data
            st.success("System metrics refreshed.")
            st.rerun() # Rerun to display new data
        else:
             st.session_state.metrics_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
             st.error(f"Metrics fetch failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")

    # Display current metrics data from session state
    current_metrics = st.session_state.get("metrics_data")
    if current_metrics and not current_metrics.get("error"):
        st.markdown("### Current System Metrics:")
        st.json(current_metrics) # Display the full JSON metrics
    elif current_metrics and current_metrics.get("error"):
         st.error(f"Failed to load metrics: {current_metrics.get('details', 'Unknown error.')}")
    else:
        st.info("Click 'Fetch System Metrics' to see data.")


# === Voice Agent Section ===
# (Kept as per your last provided file - simpler version without full Gemini bi-directional audio yet)
st.markdown("---")
st.markdown("## üéôÔ∏è DebugIQ Voice Agent")
st.caption("Speak your commands to the DebugIQ Agent.")

# Display chat history
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # Assuming audio_base64 is present for assistant messages when using voice output
        if "audio_base64" in message and message["role"] == "assistant" and st.session_state.get("using_gemini_voice", False):
             try:
                 # Decode the base64 audio data
                 audio_bytes = base64.b64decode(message["audio_base64"])
                 # Provide the audio data to Streamlit's audio player
                 st.audio(audio_bytes, format="audio/mp3") # Assuming backend provides mp3
             except Exception as e:
                 logger.error(f"Error decoding or playing audio for chat message: {e}")


# WebRTC streamer for capturing audio
# Added ClientSettings import and usage as it was in your original snippet
try:
    ctx = webrtc_streamer(
        key=f"voice_agent_stream_{BACKEND_URL}", # Use backend URL in key to differentiate if needed
        mode=WebRtcMode.SENDONLY, # We only need to send audio from the browser
        client_settings=ClientSettings(
             rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}, # Example STUN server
             media_stream_constraints={"audio": True, "video": False} # Only enable audio
        ),
        # async_processing=True # Consider if audio processing is time-consuming
    )
except Exception as e:
    st.error(f"Failed to initialize voice agent: {e}")
    logger.exception("Error initializing webrtc_streamer for Voice Agent")
    ctx = None # Set ctx to None if initialization fails

# Audio processing logic when the streamer is connected and receiving audio
if ctx and ctx.audio_receiver:
    voice_status_indicator_main_agent = st.empty() # Placeholder for status messages

    # Note: The audio capturing and processing loop using `ctx.audio_receiver.get_frame()`
    # and sending to your backend for transcription and command processing is complex
    # and requires careful handling of threads, buffering, and API calls.
    # The original code snippet for this part was commented out or incomplete.
    # Below is a simplified representation based on the structure; you will need
    # to implement the actual audio capture, buffering, and sending logic here.

    # This part of the logic needs to run in a way that doesn't block the Streamlit rerun.
    # Using a separate thread for audio processing and API calls is common in Streamlit.

    voice_status_indicator_main_agent.info("Voice agent initialized. Speak now.")

    # --- Placeholder for Audio Processing and API Call Logic ---
    # You need to implement the loop that reads audio frames from ctx.audio_receiver,
    # buffers them, detects speech (optional but recommended), sends to VOICE_TRANSCRIBE_URL,
    # gets the transcription, sends commands to COMMAND_URL or GEMINI_CHAT_URL,
    # and updates st.session_state.chat_history.
    # This often involves a dedicated thread started when ctx.state.playing becomes True.

    # Example (conceptual) - Requires a separate thread implementation:
    # if ctx.state.playing:
    #      if "audio_processing_thread" not in st.session_state:
    #           st.session_state.audio_processing_thread = start_audio_processing_thread(
    #                ctx.audio_receiver,
    #                VOICE_TRANSCRIBE_URL,
    #                COMMAND_URL, # Or GEMINI_CHAT_URL
    #                st.session_state
    #           )
    #      # Thread would handle reading frames, buffering, sending to API, updating chat_history


    # To display real-time transcription feedback or status, you might need
    # mechanisms for the processing thread to communicate back to the main Streamlit thread.
    # This is an advanced Streamlit pattern.

    # --- End Placeholder ---

elif ctx and ctx.state.state == "READY":
     voice_status_indicator_main_agent = st.empty()
     voice_status_indicator_main_agent.info("Voice agent ready. Click Start.")
elif ctx:
     voice_status_indicator_main_agent = st.empty()
     voice_status_indicator_main_agent.info(f"Voice agent state: {ctx.state.state}")
else:
     # Error message already shown in the try...except block
     pass
