# dashboard.py

import streamlit as st
import requests
import os
import difflib
import tempfile
from streamlit_ace import st_ace
from streamlit_webrtc import webrtc_streamer, WebRtcMode, ClientSettings # Import ClientSettings
import numpy as np
import av
import streamlit.components.v1 as components
import wave
import json
import logging
import base64
import re
from datetime import datetime # Added import for datetime used in backend snippet

# === Logging Setup ===
# Configure logging to see output in your terminal or deployment logs
# This logging is essential for debugging the rerun loop cause.
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Add debug logs to trace reruns and state
logger.info("--- Script Rerun Start ---")
logger.info(f"Session State Keys at start: {list(st.session_state.keys())}") # Log keys to see what's in state
logger.info(f"Initial github_repo_url_input: '{st.session_state.get('github_repo_url_input', '').strip()}'")
logger.info(f"Initial current_github_repo_url: '{st.session_state.get('current_github_repo_url')}'")
logger.info(f"Initial github_selected_branch: '{st.session_state.get('github_selected_branch')}'")
logger.info(f"Initial github_path_stack: {st.session_state.get('github_path_stack')}")


# === Constants ===
# Set the default backend URL based on your Railway deployment
DEFAULT_BACKEND_URL = "https://debugiq-backend.railway.app"
# Use environment variable BACKEND_URL if set, otherwise use the default
BACKEND_URL = os.getenv("BACKEND_URL", DEFAULT_BACKEND_URL)

# Define backend API endpoint URLs based on main.py routing
# Structure: f"{BACKEND_URL}/[prefix_from_main.py]/[path_in_router_file]"
# !!! IMPORTANT: Verify the exact paths within your router files (e.g., analyze.py, qa.py, etc.)
SUGGEST_PATCH_URL = f"{BACKEND_URL}/debugiq/suggest_patch" # Prefix /debugiq from main.py + assumed path /suggest_patch
QA_URL = f"{BACKEND_URL}/qa/run_qa" # Prefix /qa from main.py + assumed path /run_qa
DOC_URL = f"{BACKEND_URL}/doc/generate_doc" # Prefix /doc from main.py + assumed path /generate_doc
ISSUES_INBOX_URL = f"{BACKEND_URL}/issues_inbox" # No prefix + assumed path /issues_inbox
WORKFLOW_RUN_URL = f"{BACKEND_URL}/workflow/run" # Prefix /workflow from main.py + assumed path /run
WORKFLOW_CHECK_URL = f"{BACKEND_URL}/workflow/status" # Prefix /workflow from main.py + assumed path /status
METRICS_URL = f"{BACKEND_URL}/system_metrics" # No prefix + assumed path /system_metrics

# Voice/Chat related endpoints - verify paths in your voice/chat router files (e.g., voice_ws_router.py)
# Assuming these paths are defined directly without prefixes in the included voice router
COMMAND_URL = f"{BACKEND_URL}/command"
VOICE_TRANSCRIBE_URL = f"{BACKEND_URL}/transcribe_audio"
GEMINI_CHAT_URL = f"{BACKEND_URL}/gemini_chat"


DEFAULT_VOICE_SAMPLE_RATE = 16000
DEFAULT_VOICE_SAMPLE_WIDTH = 2  # Corrected space here
DEFAULT_VOICE_CHANNELS = 1  # Mono
AUDIO_PROCESSING_THRESHOLD_SECONDS = 2

TRACEBACK_EXTENSION = ".txt"
SUPPORTED_SOURCE_EXTENSIONS = (
    ".py", ".js", ".java", ".c", ".cpp", ".cs", ".go", ".rb", ".php",
    ".html", ".css", ".md", ".ts", ".tsx", ".json", ".yaml", ".yml",
    ".sh", ".R", ".swift", ".kt", ".scala"
)
RECOGNIZED_FILE_EXTENSIONS = SUPPORTED_SOURCE_EXTENSIONS + (TRACEBACK_EXTENSION,)
RECOGNIZED_FILE_EXTENSIONS = SUPPORTED_SOURCE_EXTENSIONS + (TRACEBACK_EXTENSION,)

# === Helper Functions ===

def make_api_request(method, url, json_payload=None, files=None, operation_name="API Request"):
    """
    Placeholder function to make API requests to the backend.
    Replace with your actual implementation (e.g., using requests, handling auth, etc.).
    """
    logger.info(f"Attempting to make {method} request to {url} for {operation_name}")
    try:
        response = requests.request(method, url, json=json_payload, files=files, timeout=30)
        logger.info(f"{operation_name} response status code: {response.status_code}")
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        logger.info(f"Successful {method} request to {url}")
        # Attempt to return JSON, handle cases where response might be empty or not JSON
        try:
            return response.json()
        except json.JSONDecodeError:
            logger.warning(f"Response for {url} was not JSON. Status: {response.status_code}")
            # Return a success indicator even if no JSON, if status code is 2xx
            if 200 <= response.status_code < 300:
                return {"success": True, "message": "Request successful, no JSON response."}
            else:
                return {"error": True, "details": f"Non-JSON response, status code: {response.status_code}"}

    except requests.exceptions.Timeout:
        st.error(f"â° {operation_name} failed: Request timed out. URL: {url}")
        logger.error(f"{operation_name} timed out: {url}")
        return {"error": True, "details": "Request timed out."}
    except requests.exceptions.RequestException as e:
        st.error(f"âŒ {operation_name} failed: {e}. URL: {url}")
        logger.error(f"{operation_name} failed for {url}: {e}")
        return {"error": True, "details": str(e)}


def clear_github_selection_state():
    """Resets only GitHub-related selection state in the sidebar."""
    logger.info("Clearing GitHub selection state.")
    # Do NOT clear github_repo_url_input here, let the text_input widget manage its value
    st.session_state.current_github_repo_url = None
    st.session_state.github_branches = []
    st.session_state.github_selected_branch = None
    st.session_state.github_path_stack = [""]
    st.session_state.github_repo_owner = None
    st.session_state.github_repo_name = None
    # Note: This does NOT clear analysis results
    # logger.info(f"GitHub selection state after clearing: {st.session_state.get('current_github_repo_url')}, {st.session_state.get('github_branches')}, {st.session_state.get('github_path_stack')}")


def clear_analysis_inputs():
    """Clears loaded traceback and source files used as analysis inputs."""
    logger.info("Clearing analysis inputs (trace and sources).")
    st.session_state.analysis_results['trace'] = None
    st.session_state.analysis_results['source_files_content'] = {}
    # logger.info(f"Analysis inputs state after clearing: {st.session_state.analysis_results.get('trace')}, {st.session_state.analysis_results.get('source_files_content')}")


def clear_analysis_outputs():
    """Clears generated patch, explanation, and other analysis outputs."""
    logger.info("Clearing analysis outputs (patch, explanation, etc.).")
    st.session_state.analysis_results.update({
        'patch': None,
        'explanation': None,
        'doc_summary': None,  # Assuming doc_summary comes from analysis results
        'patched_file_name': None,
        'original_patched_file_content': None
    Â })
    st.session_state.edited_patch = "" # Also clear any edited patch state
    # logger.info(f"Analysis outputs state after clearing: {st.session_state.analysis_results.get('patch')}, {st.session_state.analysis_results.get('explanation')}")


# === Session State Initialization ===
session_defaults = {
Â  Â  "audio_sample_rate": DEFAULT_VOICE_SAMPLE_RATE,
Â  Â  "audio_sample_width": DEFAULT_VOICE_SAMPLE_WIDTH,
Â  Â  "audio_num_channels": DEFAULT_VOICE_CHANNELS,
Â  Â  "audio_buffer": b"",
Â  Â  "audio_frame_count": 0,
Â  Â  "chat_history": [],
Â  Â  "edited_patch": "",
Â  Â  "github_repo_url_input": "", # Stores the value of the text input widget
Â  Â  "current_github_repo_url": None, # Stores the URL that branches were successfully loaded for
Â  Â  "github_branches": [],
Â  Â  "github_selected_branch": None,
Â  Â  "github_path_stack": [""], # Stack to manage current directory path in GitHub browser
Â  Â  "github_repo_owner": None,
Â  Â  "github_repo_name": None,
Â  Â  "analysis_results": {
Â  Â  Â  Â  "trace": None, # Loaded traceback content
Â  Â  Â  Â  "source_files_content": {}, # Dictionary of {file_path: content} for source files
Â  Â  Â  Â  "patch": None, # Generated or edited patch content
Â  Â  Â  Â  "explanation": None, # Explanation of the patch
Â  Â  Â  Â  "doc_summary": None, # Documentation summary (can come from analysis or separate doc gen)
Â  Â  Â  Â  "patched_file_name": None, # Name of the file the patch applies to
Â  Â  Â  Â  "original_patched_file_content": None # Original content of the file being patched
Â  Â  },
Â  Â  "qa_result": None, # Result of QA validation
Â  Â  "inbox_data": None, # Data from the issues inbox
Â  Â  "workflow_status_data": None, # Data from workflow status check
Â  Â  "metrics_data": None, # Data from system metrics
Â  Â  "qa_code_to_validate": None # Code specifically loaded for QA validation (maybe redundant with analysis_results['patch'])
}
for key, default_value in session_defaults.items():
Â  Â  if key not in st.session_state:
Â  Â  Â  Â  st.session_state[key] = default_value

# === Streamlit Page Configuration ===
st.set_page_config(page_title="DebugIQ Dashboard", layout="wide")
st.title("ğŸ§  DebugIQ Autonomous Debugging Dashboard")

# === GitHub Repo Integration Sidebar ===
with st.sidebar:
Â  Â  st.markdown("### ğŸ“¦ Load Code from GitHub")

Â  Â  # Text input for GitHub URL
Â  Â  # The value is automatically managed by Streamlit in st.session_state.github_repo_url_input
Â  Â  github_url_input_widget_value = st.text_input(
    "Public GitHub Repository URL",
    placeholder="https://github.com/owner/repo",
    key="github_repo_url_input",
)

if st.button(f"Load/Process Repo", key=f"load_repo_button_{github_url_input_widget_value}", use_container_width=True):
    input_url = github_url_input_widget_value.strip()
    logger.info(f"Load/Process Repo button clicked. Input URL: '{input_url}'")

    if not input_url:
        # Clear GitHub state and reset analysis results if the input is empty
        logger.info("Input URL is empty. Clearing GitHub state and analysis results.")
        clear_github_selection_state()
        clear_analysis_inputs()
        clear_analysis_outputs()
        st.info("GitHub input cleared and analysis reset.")
        st.session_state.current_github_repo_url = None
    elif not input_url.startswith("PROCESSING_") and input_url != st.session_state.current_github_repo_url:
        # Set processing marker for new valid input and rerun
        logger.info(f"New valid URL detected: '{input_url}'. Preparing to process.")
        clear_analysis_inputs()
        clear_analysis_outputs()
        st.session_state.current_github_repo_url = "PROCESSING_" + input_url
        st.rerun()
    else:
        logger.warning(f"Button clicked but no valid state change detected. Current URL: {st.session_state.current_github_repo_url}")

# Check if the URL needs processing
current_loaded_url = st.session_state.get("current_github_repo_url")
if current_loaded_url and current_loaded_url.startswith("PROCESSING_"):
    # Extract the actual URL from the processing marker
    active_github_url = current_loaded_url.replace("PROCESSING_", "").strip()
    logger.info(f"Processing GitHub URL: {active_github_url}")

    # Validate and parse the URL
    match = re.match(r"https://github\.com/([^/]+)/([^/]+?)(?:\.git)?$", active_github_url)
    if not match:
        st.warning("Invalid GitHub URL format. Use: https://github.com/owner/repo")
        clear_github_selection_state()
        st.session_state.current_github_repo_url = None
        logger.warning("Reset current_github_repo_url to None due to invalid URL.")
    else:
        # Process the valid URL (e.g., fetch branches)
        owner, repo = match.groups()
        logger.info(f"Valid URL parsed: owner={owner}, repo={repo}")

        # Fetch branches or perform necessary processing
        try:
            api_branches_url = f"https://api.github.com/repos/{owner}/{repo}/branches"
            with st.spinner(f"Loading branches for {owner}/{repo}..."):
                branches_res = requests.get(api_branches_url, timeout=10)
                branches_res.raise_for_status()
                branches_data = branches_res.json()
                st.session_state.github_branches = [b["name"] for b in branches_data]
            logger.info(f"Branch fetch successful. Found {len(st.session_state.github_branches)} branches.")

            # Update the state to mark processing complete
            st.session_state.current_github_repo_url = active_github_url
            st.success(f"Repo '{owner}/{repo}' branches loaded.")
        except Exception as e:
            logger.error(f"Error processing GitHub URL: {e}")
            st.error(f"Failed to process GitHub URL: {e}")
            clear_github_selection_state()
            st.session_state.current_github_repo_url = NoneÂ  Â  # --- GitHub URL Parsing & Branch Fetch ---
Â  Â  # This logic runs on every rerun and is triggered when github_repo_url_input changes
Â  Â  # and the Load/Process Repo button's logic sets current_github_repo_url to trigger it.
Â  Â  active_github_url = st.session_state.get("github_repo_url_input", "").strip()
Â  Â  current_loaded_url = st.session_state.get("current_github_repo_url")

Â  Â  logger.info(f"Branch fetch check start. active_github_url='{active_github_url}', current_loaded_url='{current_loaded_url}'")
Â  Â  # The condition to trigger branch fetching:
Â  Â  # 1. There's an active URL in the input
Â  Â  # 2. We haven't successfully loaded branches for this exact URL yet (current_loaded_url is None or different)
Â  Â  # 3. Avoid triggering if current_loaded_url is the "PROCESSING_" marker set by the button
Â  Â  condition_met = active_github_url and (current_loaded_url is None or (isinstance(current_loaded_url, str) and not current_loaded_url.startswith("PROCESSING_") and current_loaded_url != active_github_url))
Â  Â  logger.info(f"Branch fetch condition met: {condition_met}")

Â  Â  if condition_met:
Â  Â  Â  Â  Â match = re.match(r"https://github\.com/([^/]+)/([^/]+?)(?:\.git)?$", active_github_url)
Â  Â  Â  Â  Â if not match:
Â  Â  Â  Â  Â  Â  Â logger.warning(f"Invalid GitHub URL format in fetch check: {active_github_url}")
Â  Â  Â  Â  Â  Â  Â st.warning("Invalid GitHub URL format. Use: https://github.com/owner/repo")
Â  Â  Â  Â  Â  Â  Â clear_github_selection_state() # Clear GitHub states on invalid format
Â  Â  Â  Â  Â  Â  Â st.session_state.current_github_repo_url = None # Ensure loaded URL is explicitly None
Â  Â  Â  Â  Â  Â  Â logger.info(f"Reset current_github_repo_url to None due to invalid URL format.")
Â  Â  Â  Â  Â else:
Â  Â  Â  Â  Â  Â  Â owner, repo = match.groups()
Â  Â  Â  Â  Â  Â  Â logger.info(f"Valid URL parsed: owner={owner}, repo={repo}")

Â  Â  Â  Â  Â  Â  Â # Update owner/repo names
Â  Â  Â  Â  Â  Â  Â st.session_state.github_repo_owner = owner
Â  Â  Â  Â  Â  Â  Â st.session_state.github_repo_name = repo

Â  Â  Â  Â  Â  Â  Â api_branches_url = f"https://api.github.com/repos/{owner}/{repo}/branches"
Â  Â  Â  Â  Â  Â  Â logger.info(f"Attempting branch fetch for: {api_branches_url}")

Â  Â  Â  Â  Â  Â  Â try:
Â  Â  Â  Â  Â  Â  Â  Â  Â with st.spinner(f"Loading branches for {owner}/{repo}..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â branches_res = requests.get(api_branches_url, timeout=10)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â branches_res.raise_for_status()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â branches_data = branches_res.json()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.github_branches = [b["name"] for b in branches_data]
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info(f"Branch fetch successful. Found {len(st.session_state.github_branches)} branches.")

Â  Â  Â  Â  Â  Â  Â  Â  Â if st.session_state.github_branches:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Set branch selection to the first branch if none selected or old selection invalid
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if st.session_state.github_selected_branch not in st.session_state.github_branches:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.github_selected_branch = st.session_state.github_branches[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.github_path_stack = [""] # Reset path on successful repo load
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.success(f"Repo '{owner}/{repo}' branches loaded.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # --- Crucial Update for Loop Prevention ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Mark the URL as successfully loaded *only after successful API call and branches found*
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.current_github_repo_url = active_github_url
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info(f"Branch fetch successful. Set current_github_repo_url to: '{st.session_state.current_github_repo_url}'")
Â  Â  Â  Â  Â  Â  Â  Â  Â else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.warning("No branches found for this repository.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â clear_github_selection_state() # Clear GitHub states if no branches
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.current_github_repo_url = None # Reset on no branches found
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.warning("Branch fetch found no branches. Reset current_github_repo_url to None.")


Â  Â  Â  Â  Â  Â  Â except requests.exceptions.RequestException as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error(f"âŒ Branch fetch API error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"âŒ Branch fetch error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â clear_github_selection_state() # Clear GitHub states on error
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.current_github_repo_url = None # Reset on API error to allow retry
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.warning("Branch fetch API error. Reset current_github_repo_url to None.")

Â  Â  Â  Â  Â  Â  Â except json.JSONDecodeError as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Branch JSON decode error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Branch JSON error: Could not parse response. {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  clear_github_selection_state() # Clear GitHub states on error
Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_github_repo_url = None # Reset on JSON error to allow retry
Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("Branch fetch JSON error. Reset current_github_repo_url to None.")

Â  Â  # --- Branch Selection ---
Â  Â  # Only show branch selector if branches were successfully loaded
Â  Â  current_branches = st.session_state.get("github_branches", [])
Â  Â  if current_branches:
Â  Â  Â  Â  logger.info(f"Showing branch selector. {len(current_branches)} branches available.")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # Find the index of the currently selected branch, default to 0 if not found
Â  Â  Â  Â  Â  Â  branch_idx = current_branches.index(st.session_state.github_selected_branch) if st.session_state.github_selected_branch in current_branches else 0
Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  # Fallback in case of unexpected value, should be covered by above check
Â  Â  Â  Â  Â  Â  branch_idx = 0
Â  Â  Â  Â  logger.info(f"Selected branch index: {branch_idx}")

Â  Â  Â  Â  selected_branch = st.selectbox(
Â  Â  Â  Â  Â  Â  "Select Branch",
Â  Â  Â  Â  Â  Â  current_branches,
Â  Â  Â  Â  Â  Â  index=branch_idx,
Â  Â  Â  Â  Â  Â  key="github_branch_selector",
Â  Â  Â  Â  Â  Â  on_change=lambda: logger.info(f"Branch selector changed to {st.session_state.github_selected_branch}. Resetting path stack.") or st.session_state.update({"github_path_stack": [""]}) # Reset path on branch change
Â  Â  Â  Â  )
Â  Â  Â  Â  # Streamlit automatically updates st.session_state.github_selected_branch due to the key

Â  Â  # --- File Browser Logic (if owner, repo, branch are set) ---
Â  Â  gh_owner = st.session_state.get("github_repo_owner")
Â  Â  gh_repo = st.session_state.get("github_repo_name")
Â  Â  gh_branch = st.session_state.get("github_selected_branch")
Â  Â  current_path_stack = st.session_state.get("github_path_stack", [""])
Â  Â  current_path_str = "/".join([p for p in current_path_stack if p])

Â  Â  if gh_owner and gh_repo and gh_branch:
Â  Â  Â  Â  logger.info(f"File browser active for {gh_owner}/{gh_repo} @ {gh_branch} path /{current_path_str}")

Â  Â  Â  Â  @st.cache_data(ttl=120, show_spinner="Fetching directory contents...")
Â  Â  Â  Â  def fetch_gh_dir_content(owner_str, repo_str, path_str, branch_str):
Â  Â  Â  Â  Â  Â  url = f"https://api.github.com/repos/{owner_str}/{repo_str}/contents/{path_str}?ref={branch_str}"
Â  Â  Â  Â  Â  Â  logger.info(f"Fetching GitHub dir (cached): {url}")
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  r = requests.get(url, timeout=10)
Â  Â  Â  Â  Â  Â  Â  Â  r.raise_for_status()
Â  Â  Â  Â  Â  Â  Â  Â  return r.json()
Â  Â  Â  Â  Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Dir content fetch error for '{path_str}': {e}")
Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Dir content fetch error for '{path_str}': {e}")
Â  Â  Â  Â  Â  Â  except json.JSONDecodeError as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Dir content JSON error for '{path_str}': {e}")
Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Dir content JSON error for '{path_str}': {e}")
Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  entries = fetch_gh_dir_content(gh_owner, gh_repo, current_path_str, gh_branch)

Â  Â  Â  Â  if entries is not None:
Â  Â  Â  Â  Â  Â  logger.info(f"Fetched {len(entries)} items for path /{current_path_str}")
Â  Â  Â  Â  Â  Â  # Display current path and "Up" button
Â  Â  Â  Â  Â  Â  display_path = f"Current Path: /{current_path_str}" if current_path_str else "Current Path: / (Repo Root)"
Â  Â  Â  Â  Â  Â  st.caption(display_path)
Â  Â  Â  Â  Â  Â  if len(current_path_stack) > 1: # Not in root
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("â¬†ï¸ .. (Parent Directory)", key=f"gh_up_button_{current_path_str}", use_container_width=True):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.github_path_stack.pop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Clicked 'Up' button. New path stack: {st.session_state.github_path_stack}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  st.markdown("###### Directories")
Â  Â  Â  Â  Â  Â  # Filter and sort directories
Â  Â  Â  Â  Â  Â  dirs = sorted([item for item in entries if item["type"] == "dir"], key=lambda x: x["name"])
Â  Â  Â  Â  Â  Â  for item in dirs:
Â  Â  Â  Â  Â  Â  Â  Â  Â if st.button(f"ğŸ“ {item['name']}", key=f"gh_dir_{current_path_str}_{item['name']}", use_container_width=True):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.github_path_stack.append(item['name'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info(f"Clicked directory '{item['name']}'. New path stack: {st.session_state.github_path_stack}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun()

Â  Â  Â  Â  Â  Â  st.markdown("###### Files")
Â  Â  Â  Â  Â  Â  # Filter and sort recognized files
Â  Â  Â  Â  Â  Â  files = sorted([item for item in entries if item["type"] == "file" and any(item["name"].endswith(ext) for ext in RECOGNIZED_FILE_EXTENSIONS)], key=lambda x: x["name"])
Â  Â  Â  Â  Â  Â  for item in files:
Â  Â  Â  Â  Â  Â  Â  Â  if st.button(f"ğŸ“„ {item['name']}", key=f"gh_file_{current_path_str}_{item['name']}", use_container_width=True):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_rel_path = f"{current_path_str}/{item['name']}".strip("/")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw_url = f"https://raw.githubusercontent.com/{gh_owner}/{gh_repo}/{gh_branch}/{file_rel_path}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Clicked file '{item['name']}'. Attempting to fetch: {raw_url}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"Loading {item['name']}..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_r = requests.get(raw_url, timeout=10)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_r.raise_for_status()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_content = file_r.text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Successfully fetched file '{item['name']}'. Size: {len(file_content)} bytes.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use relative path from repo root as key for analysis_results
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key_path = os.path.join(*current_path_stack, item['name']).replace("\\","/") if current_path_str else item['name']

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if item['name'].endswith(TRACEBACK_EXTENSION):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.analysis_results['trace'] = file_content
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"'{key_path}' loaded as Traceback.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # When loading a new traceback, clear previous analysis outputs
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  clear_analysis_outputs()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("Cleared analysis outputs after loading new traceback.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: # Already filtered by RECOGNIZED_FILE_EXTENSIONS
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.analysis_results['source_files_content'][key_path] = file_content
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"'{key_path}' loaded as Source File.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("Added file content to analysis inputs.")


Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # To update "Loaded Analysis Inputs" expander and potentially other tabs
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error loading file '{item['name']}': {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Error fetching file '{item['name']}': {e}")
Â  Â  Â  Â  Â  Â  if not dirs and not files:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.caption("No directories or recognized files found in this path.")

Â  Â  Â  Â  elif current_path_str: # entries is None and not in root, indicating a fetch error
Â  Â  Â  Â  Â  Â  Â st.warning("Could not list directory contents.")
Â  Â  Â  Â  Â  Â  Â logger.warning(f"Could not fetch directory contents for path /{current_path_str}. Entries is None.")

Â  Â  st.markdown("---")
Â  Â  with st.expander("ğŸ“¬ Loaded Analysis Inputs", expanded=True):
Â  Â  Â  Â  trace_val = st.session_state.analysis_results.get('trace')
Â  Â  Â  Â  sources_val = st.session_state.analysis_results.get('source_files_content', {})
Â  Â  Â  Â  if trace_val: st.text_area("Current Traceback:", value=trace_val, height=100, disabled=True, key="sidebar_trace_loaded_final")
Â  Â  Â  Â  else: st.caption("No traceback loaded.")
Â  Â  Â  Â  if sources_val:
Â  Â  Â  Â  Â  Â  st.write("Current Source Files:")
Â  Â  Â  Â  Â  Â  for name in sources_val.keys(): st.caption(f"- {name}")
Â  Â  Â  Â  else: st.caption("No source files loaded.")
Â  Â  st.markdown("---")

# === Manual File Uploader (Main Page Area) ===
st.markdown("### â¬†ï¸ Or, Upload Files Manually")
manual_uploaded_files_main = st.file_uploader(
Â  Â  "ğŸ“„ Upload traceback (.txt) and/or source files",
Â  Â  type=[ext.lstrip('.') for ext in RECOGNIZED_FILE_EXTENSIONS],
Â  Â  accept_multiple_files=True,
Â  Â  key="manual_uploader_main_page"
)

if manual_uploaded_files_main:
Â  Â  logger.info("Manual uploader detected files in this rerun.")
Â  Â  manual_trace_str = None
Â  Â  manual_sources_dict = {}
Â  Â  manual_summary_list_main = []

Â  Â  # Process uploaded files
Â  Â  for file_item in manual_uploaded_files_main:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # Read file content
Â  Â  Â  Â  Â  Â  # file_item.getvalue() returns bytes, decode using utf-8
Â  Â  Â  Â  Â  Â  content_bytes = file_item.getvalue()
Â  Â  Â  Â  Â  Â  content_str = content_bytes.decode("utf-8", errors='ignore') # Use errors='ignore' for robustness

Â  Â  Â  Â  Â  Â  if file_item.name.endswith(TRACEBACK_EXTENSION):
Â  Â  Â  Â  Â  Â  Â  Â  manual_trace_str = content_str
Â  Â  Â  Â  Â  Â  Â  Â  manual_summary_list_main.append(f"Traceback: {file_item.name}")
Â  Â  Â  Â  Â  Â  elif any(file_item.name.endswith(ext) for ext in SUPPORTED_SOURCE_EXTENSIONS):
Â  Â  Â  Â  Â  Â  Â  Â  manual_sources_dict[file_item.name] = content_str # Use file name as key
Â  Â  Â  Â  Â  Â  Â  Â  manual_summary_list_main.append(f"Source: {file_item.name}")
Â  Â  Â  Â  Â  Â  # Ignore files with unrecognized extensions
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Error processing '{file_item.name}': {e}")
Â  Â  Â  Â  Â  Â  logger.error(f"Error processing uploaded file {file_item.name}: {e}")


Â  Â  # Decide how to update state based on what was uploaded
Â  Â  # If files were uploaded, clear the GitHub selection state
Â  Â  logger.info("Clearing GitHub selection state due to manual upload.")
Â  Â  clear_github_selection_state()

Â  Â  if manual_trace_str is not None:
Â  Â  Â  Â  # If a traceback is uploaded, clear all previous analysis data (from GitHub or prior uploads)
Â  Â  Â  Â  # and set the new traceback and any source files uploaded alongside it.
Â  Â  Â  Â  logger.info("Traceback found in manual upload. Clearing analysis state.")
Â  Â  Â  Â  clear_analysis_inputs()
Â  Â  Â  Â  clear_analysis_outputs()
Â  Â  Â  Â  st.session_state.analysis_results['trace'] = manual_trace_str
Â  Â  Â  Â  st.session_state.analysis_results['source_files_content'] = manual_sources_dict # Only sources from this upload
Â  Â  Â  Â  st.success(f"Manually uploaded: {', '.join(manual_summary_list_main)}. GitHub selection cleared.")
Â  Â  Â  Â  st.rerun() # Rerun to update UI with new analysis inputs
Â  Â  elif manual_sources_dict:
Â  Â  Â  Â  Â # If only source files are uploaded (and no traceback in this upload),
Â  Â  Â  Â  Â # merge them into the existing source files state. Keep existing traceback.
Â  Â  Â  Â  Â logger.info("Source files found in manual upload (no traceback). Merging sources.")
Â  Â  Â  Â  Â st.session_state.analysis_results['source_files_content'].update(manual_sources_dict)
Â  Â  Â  Â  Â st.success(f"Manually added source files: {', '.join(manual_summary_list_main)}. GitHub selection cleared.")
Â  Â  Â  Â  Â st.rerun() # Rerun to update UI with new source files
Â  Â  else:
Â  Â  Â  Â  logger.warning("Manual uploader files detected, but no recognized trace or sources found.")
Â  Â  # If no recognized files, no rerun is triggered by this block, which is correct.


# === Main Application Tabs ===
tab_titles_main_list = [
Â  Â  "ğŸ“„ Traceback + Patch", "âœ… QA Validation", "ğŸ“˜ Documentation",
Â  Â  "ğŸ“£ Issue Notices", "ğŸ¤– Autonomous Workflow", "ğŸ” Workflow Check",
Â  Â  "ğŸ“Š Repo Structure Insights",
Â  Â  "ğŸ“ˆ Metrics"
]
# Assign tabs to variables
tab_patch, tab_qa, tab_doc, tab_issues, tab_workflow_trigger, \
Â  Â  tab_workflow_status, tab_repo_insights, tab_metrics = st.tabs(tab_titles_main_list)

# Content for each tab - this code runs on every rerun, but the content is only displayed
# for the active tab. State accessed within these blocks reflects the current session state.

with tab_patch:
Â  Â  st.header("ğŸ“„ Traceback & Patch Analysis")
Â  Â  loaded_trace_tab1 = st.session_state.analysis_results.get('trace')
Â  Â  loaded_sources_tab1 = st.session_state.analysis_results.get('source_files_content', {})
Â  Â  patched_code_val_tab1 = st.session_state.analysis_results.get('patch')
Â  Â  original_content_tab1 = st.session_state.analysis_results.get('original_patched_file_content')
Â  Â  explanation_val_tab1 = st.session_state.analysis_results.get('explanation')

Â  Â  st.markdown("### Analysis Inputs")
Â  Â  if loaded_trace_tab1:
Â  Â  Â  Â  st.text_area("Loaded Traceback:", value=loaded_trace_tab1, height=150, disabled=True)
Â  Â  else:
Â  Â  Â  Â  st.info("Load a traceback from GitHub or manually upload to enable analysis.")

Â  Â  if loaded_sources_tab1:
Â  Â  Â  Â  with st.expander("Loaded Source Files:", expanded=False):
Â  Â  Â  Â  Â  Â  Â st.json(loaded_sources_tab1)
Â  Â  else:
Â  Â  Â  Â  Â st.info("Load source files from GitHub or manually upload to provide context for analysis.")


Â  Â  if st.button("ğŸ”¬ Analyze Loaded Data & Suggest Patch", key="tab1_analyze_main_btn", use_container_width=True):
Â  Â  Â  Â  logger.info("Analyze button clicked in Patch tab.")
Â  Â  Â  Â  # This button triggers analysis based on the CURRENT inputs in session state
Â  Â  Â  Â  # It should populate the analysis OUTPUTS state.
Â  Â  Â  Â  if not loaded_trace_tab1 and not loaded_sources_tab1:
Â  Â  Â  Â  Â  Â  st.warning("Please load data (traceback and/or source files) first from the sidebar or uploader.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # Prepare payload from current session state inputs
Â  Â  Â  Â  Â  Â  payload = {
Â  Â  Â  Â  Â  Â  Â  Â  "trace": loaded_trace_tab1,
Â  Â  Â  Â  Â  Â  Â  Â  "language": "python", # Assuming language is always python for this example, adjust if needed
Â  Â  Â  Â  Â  Â  Â  Â  "source_files": loaded_sources_tab1
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  # Call your backend API for patch suggestion
Â  Â  Â  Â  Â  Â  response = make_api_request("POST", SUGGEST_PATCH_URL, json_payload=payload, operation_name="Patch Suggestion")

Â  Â  Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â # Update only the analysis OUTPUTS in session state based on the API response
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.analysis_results.update({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'patch': response.get("patched_code", ""),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'explanation': response.get("explanation"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'patched_file_name': response.get("patched_file_name"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'original_patched_file_content': response.get("original_content")
Â  Â  Â  Â  Â  Â  Â  Â  Â })
Â  Â  Â  Â  Â  Â  Â  Â  Â # Initialize the edited patch state with the suggested patch
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.edited_patch = st.session_state.analysis_results['patch']
Â  Â  Â  Â  Â  Â  Â  Â  Â st.success("Patch suggestion received.")
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info("Successfully received patch suggestion.")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun() # Rerun to update UI with results, diff, and editor
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â # If analysis fails, clear the previous outputs and show an error
Â  Â  Â  Â  Â  Â  Â  Â  Â clear_analysis_outputs()
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"Patch analysis failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error("Patch analysis failed.")


Â  Â  st.markdown("---")
Â  Â  st.markdown("### Analysis Results")

Â  Â  if patched_code_val_tab1 is not None:
Â  Â  Â  Â  st.markdown("### ğŸ” Diff View")
Â  Â  Â  Â  # Check if original content is available and different from the patch
Â  Â  Â  Â  if original_content_tab1 is not None and original_content_tab1 != patched_code_val_tab1:
Â  Â  Â  Â  Â  Â  html_diff_gen_tab1 = difflib.HtmlDiff(wrapcolumn=70, tabsize=4)
Â  Â  Â  Â  Â  Â  diff_html = html_diff_gen_tab1.make_table(
Â  Â  Â  Â  Â  Â  Â  Â  original_content_tab1.splitlines(keepends=True),
Â  Â  Â  Â  Â  Â  Â  Â  patched_code_val_tab1.splitlines(keepends=True),
Â  Â  Â  Â  Â  Â  Â  Â  "Original",
Â  Â  Â  Â  Â  Â  Â  Â  "Patched",
Â  Â  Â  Â  Â  Â  Â  Â  context=True,
Â  Â  Â  Â  Â  Â  Â  Â  numlines=3
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  components.html(diff_html, height=450, scrolling=True)
Â  Â  Â  Â  elif original_content_tab1 is not None and original_content_tab1 == patched_code_val_tab1:
Â  Â  Â  Â  Â  Â  Â st.info("No changes in the suggested patch compared to the original file content.")
Â  Â  Â  Â  elif patched_code_val_tab1 is not None:
Â  Â  Â  Â  Â  Â  Â st.info("Original file content not available to generate a diff.")


Â  Â  Â  Â  st.markdown("### âœï¸ Edit Patch (Live)")
Â  Â  Â  Â  # Use st_ace for live editing, its value is automatically stored in st.session_state.edited_patch
Â  Â  Â  Â  edited_code_val_tab1 = st_ace(
Â  Â  Â  Â  Â  Â  value=st.session_state.edited_patch, # Use the edited state as the source of truth for the editor
Â  Â  Â  Â  Â  Â  language="python", # Set language highlighting
Â  Â  Â  Â  Â  Â  theme="monokai", # Set editor theme
Â  Â  Â  Â  Â  Â  height=300, # Set editor height
Â  Â  Â  Â  Â  Â  key="tab1_ace_editor_main" # Key to store the value in session state
Â  Â  Â  Â  )

Â  Â  Â  Â  # If the user changed the code in the editor, update the 'patch' in analysis_results
Â  Â  Â  Â  # This comparison happens on every rerun
Â  Â  Â  Â  if edited_code_val_tab1 != st.session_state.analysis_results.get('patch'):
Â  Â  Â  Â  Â  Â  Â logger.info("st_ace editor value changed. Updating patch in session state.")
Â  Â  Â  Â  Â  Â  Â st.session_state.analysis_results['patch'] = edited_code_val_tab1
Â  Â  Â  Â  Â  Â  Â st.session_state.edited_patch = edited_code_val_tab1 # Keep edited_patch in sync
Â  Â  Â  Â  Â  Â  Â # No rerun needed here, the editor change itself triggers a rerun


Â  Â  Â  Â  if explanation_val_tab1:
Â  Â  Â  Â  Â  Â  Â st.markdown(f"**Explanation:** {explanation_val_tab1}")

Â  Â  elif loaded_trace_tab1 or loaded_sources_tab1:
Â  Â  Â  Â  Â st.info("Run analysis above to generate a patch.")
Â  Â  else:
Â  Â  Â  Â  Â st.info("Load data and run analysis to see results here.")


with tab_qa:
Â  Â  st.header("âœ… QA Validation")
Â  Â  # Get the code for QA, preferably the edited patch if available, otherwise the suggested patch
Â  Â  code_for_qa_tab2 = st.session_state.get('edited_patch') or st.session_state.analysis_results.get('patch')
Â  Â  patched_file_name_tab2 = st.session_state.analysis_results.get('patched_file_name')

Â  Â  if code_for_qa_tab2 is not None:
Â  Â  Â  Â  st.text_area("Code for QA:", value=code_for_qa_tab2, height=200, disabled=True, key="qa_code_display_tab2_main_val")

Â  Â  Â  Â  if st.button("ğŸ›¡ï¸ Run QA Validation", key="qa_run_validation_btn_tab2_val", use_container_width=True):
Â  Â  Â  Â  Â  Â  logger.info("Run QA Validation button clicked.")
Â  Â  Â  Â  Â  Â  if not code_for_qa_tab2:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.warning("No code available to validate.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â payload = {"code": code_for_qa_tab2, "patched_file_name": patched_file_name_tab2}
Â  Â  Â  Â  Â  Â  Â  Â  Â # Call your backend API for QA validation
Â  Â  Â  Â  Â  Â  Â  Â  Â response = make_api_request("POST", QA_URL, json_payload=payload, operation_name="QA Validation")
Â  Â  Â  Â  Â  Â  Â  Â  Â if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.qa_result = response # Store QA result in session state
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.success("QA Validation complete.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info("Successfully received QA validation result.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun() # Rerun to display result
Â  Â  Â  Â  Â  Â  Â  Â  Â else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.qa_result = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"QA Validation failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error("QA Validation failed.")


Â  Â  Â  Â  # Display the QA result if available in session state
Â  Â  Â  Â  qa_result_val_tab2 = st.session_state.get("qa_result")
Â  Â  Â  Â  if qa_result_val_tab2:
Â  Â  Â  Â  Â  Â  st.markdown("### QA Result:")
Â  Â  Â  Â  Â  Â  if qa_result_val_tab2.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"QA Error: {qa_result_val_tab2.get('details', 'Unknown error.')}")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.json(qa_result_val_tab2) # Display the full JSON result
Â  Â  Â  Â  elif st.session_state.get("qa_result") is not None: # Handle case where result was explicitly set to None
Â  Â  Â  Â  Â  Â  Â st.info("No QA result available yet. Run validation.")


Â  Â  else:
Â  Â  Â  Â  st.warning("No patched code from the 'Traceback + Patch' tab to validate.")


with tab_doc:
Â  Â  st.header("ğŸ“˜ Documentation")
Â  Â  # Display documentation summary from analysis results if available
Â  Â  doc_summary_main_val = st.session_state.analysis_results.get('explanation') # Often explanation includes documentation aspects
Â  Â  if doc_summary_main_val:
Â  Â  Â  Â  st.markdown(f"**Documentation Summary from Analysis:** {doc_summary_main_val}")
Â  Â  Â  Â  st.markdown("---")

Â  Â  st.subheader("Generate Documentation for Specific Code:")
Â  Â  # Text area for ad-hoc documentation generation
Â  Â  doc_code_input_tab3_main_val = st.text_area("Paste code here:", key="doc_code_input_area_tab3_main_val", height=200)

Â  Â  if st.button("ğŸ“ Generate Ad-hoc Documentation", key="doc_generate_btn_tab3_main_val", use_container_width=True):
Â  Â  Â  Â  logger.info("Generate Ad-hoc Documentation button clicked.")
Â  Â  Â  Â  if not doc_code_input_tab3_main_val:
Â  Â  Â  Â  Â  Â  st.warning("Please paste code into the text area to generate documentation.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  payload = {"code": doc_code_input_tab3_main_val}
Â  Â  Â  Â  Â  Â  # Call your backend API for documentation generation
Â  Â  Â  Â  Â  Â  response = make_api_request("POST", DOC_URL, json_payload=payload, operation_name="Ad-hoc Docs")

Â  Â  Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â # Display the generated documentation
Â  Â  Â  Â  Â  Â  Â  Â  Â generated_doc = response.get("doc", "No documentation generated.")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.markdown("### Generated Documentation:")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.markdown(generated_doc)
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info("Successfully received ad-hoc documentation.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"Documentation generation failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error("Ad-hoc documentation generation failed.")


with tab_issues:
Â  Â  st.header("ğŸ“£ Issue Notices & Agent Summaries")
Â  Â  if st.button("ğŸ”„ Refresh Issue Notices", key="fetch_issues_tab_btn_val", use_container_width=True):
Â  Â  Â  Â  logger.info("Refresh Issue Notices button clicked.")
Â  Â  Â  Â  # Call your backend API to fetch issues inbox data
Â  Â  Â  Â  response = make_api_request("GET", ISSUES_INBOX_URL, operation_name="Fetch Issues")
Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  st.session_state.inbox_data = response # Store inbox data in session state
Â  Â  Â  Â  Â  Â  st.success("Issue notices refreshed.")
Â  Â  Â  Â  Â  Â  logger.info("Successfully fetched issue notices.")
Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to display new data
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.session_state.inbox_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
Â  Â  Â  Â  Â  Â  st.error(f"Issue fetch failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  logger.error("Failed to fetch issue notices.")

Â  Â  # Display current inbox data from session state
Â  Â  current_inbox = st.session_state.get("inbox_data")
Â  Â  if current_inbox and not current_inbox.get("error"):
Â  Â  Â  Â  issues_list = current_inbox.get("issues", [])
Â  Â  Â  Â  if issues_list:
Â  Â  Â  Â  Â  Â  st.markdown("### Current Issues:")
Â  Â  Â  Â  Â  Â  for i, issue_item in enumerate(issues_list):
Â  Â  Â  Â  Â  Â  Â  Â  with st.expander(f"Issue {issue_item.get('id','N/A')} - {issue_item.get('classification','N/A')}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.json(issue_item)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Button to trigger workflow for a specific issue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button(f"â–¶ï¸ Trigger Workflow for Issue {issue_item.get('id','N/A')}", key=f"trigger_issue_{issue_item.get('id',i)}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Trigger Workflow button clicked for issue {issue_item.get('id','N/A')}.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Call your backend API to trigger workflow
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  payload = {"issue_id": issue_item.get('id')}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  workflow_response = make_api_request("POST", WORKFLOW_RUN_URL, json_payload=payload, operation_name=f"Manual Workflow Trigger {issue_item.get('id','N/A')}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if workflow_response and not workflow_response.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Workflow for Issue {issue_item.get('id','N/A')} triggered successfully.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Workflow for issue {issue_item.get('id','N/A')} triggered successfully.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Optionally clear workflow status to force refresh on next tab check
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.workflow_status_data = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to update status tab potentially
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Failed to trigger workflow for Issue {issue_item.get('id','N/A')}. {workflow_response.get('details', '') if workflow_response else 'No response'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Failed to trigger workflow for issue {issue_item.get('id','N/A')}.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("No issues found in the inbox.")
Â  Â  elif current_inbox and current_inbox.get("error"):
Â  Â  Â  Â  Â st.error(f"Failed to load issues: {current_inbox.get('details', 'Unknown error.')}")
Â  Â  else:
Â  Â  Â  Â  st.info("Click 'Refresh Issue Notices' to fetch data.")


with tab_workflow_trigger:
Â  Â  st.header("ğŸ¤– Trigger Autonomous Workflow by ID")
Â  Â  workflow_issue_id_input_val = st.text_input("Enter Issue ID to trigger workflow:", key="workflow_issue_id_trigger_input_val")
Â  Â  if st.button("â–¶ï¸ Run Workflow by ID", key="run_workflow_by_id_btn_val", use_container_width=True):
Â  Â  Â  Â  logger.info(f"Run Workflow by ID button clicked for ID: {workflow_issue_id_input_val}")
Â  Â  Â  Â  if not workflow_issue_id_input_val:
Â  Â  Â  Â  Â  Â  st.warning("Please enter an Issue ID to trigger a workflow.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  payload = {"issue_id": workflow_issue_id_input_val}
Â  Â  Â  Â  Â  Â  # Call your backend API to trigger workflow by ID
Â  Â  Â  Â  Â  Â  response = make_api_request("POST", WORKFLOW_RUN_URL, json_payload=payload, operation_name=f"Manual Workflow Trigger {workflow_issue_id_input_val}")
Â  Â  Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  Â  Â  Â st.success(f"Workflow for Issue {workflow_issue_id_input_val} triggered successfully.")
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info(f"Workflow for issue {workflow_issue_id_input_val} triggered successfully.")
Â  Â  Â  Â  Â  Â  Â  Â  Â # Optionally clear workflow status to force refresh on next tab check
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.workflow_status_data = None
Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun() # Rerun to update status tab potentially
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"Failed to trigger workflow for Issue {workflow_issue_id_input_val}. {response.get('details', '') if response else 'No response'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error(f"Failed to trigger workflow for issue {workflow_issue_id_input_val}.")

with tab_workflow_status:
Â  Â  st.header("ğŸ” Workflow Status Check")
Â  Â  if st.button("ğŸ”„ Refresh Workflow Status", key="refresh_workflow_status_check_btn_val", use_container_width=True):
Â  Â  Â  Â  logger.info("Refresh Workflow Status button clicked.")
Â  Â  Â  Â  # Call your backend API to fetch workflow status
Â  Â  Â  Â  response = make_api_request("GET", WORKFLOW_CHECK_URL, operation_name="Workflow Status")
Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  st.session_state.workflow_status_data = response # Store status data
Â  Â  Â  Â  Â  Â  st.success("Workflow status refreshed.")
Â  Â  Â  Â  Â  Â  logger.info("Successfully fetched workflow status.")
Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to display new data
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.session_state.workflow_status_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
Â  Â  Â  Â  Â  Â  st.error(f"Workflow status check failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  logger.error("Failed to fetch workflow status.")

Â  Â  # Display current workflow status data from session state
Â  Â  current_status = st.session_state.get("workflow_status_data")
Â  Â  if current_status and not current_status.get("error"):
Â  Â  Â  Â  st.markdown("### Current Workflow Status:")
Â  Â  Â  Â  st.json(current_status) # Display the full JSON status
Â  Â  elif current_status and current_status.get("error"):
Â  Â  Â  Â  Â st.error(f"Failed to load workflow status: {current_status.get('details', 'Unknown error.')}")
Â  Â  else:
Â  Â  Â  Â  st.info("Click 'Refresh Workflow Status' to see data.")

with tab_repo_insights:
Â  Â  st.header("ğŸ“Š Repository Structure Insights")
Â  Â  st.markdown("""
Â  Â  This tab is intended to display a "digital chart" or visual representation of your loaded GitHub repository's content.

Â  Â  if gh_owner_insight and gh_repo_insight:
Â  Â  Â  Â  st.info(f"Insights for repository: {gh_owner_insight}/{gh_repo_insight} would appear here once specified.")
Â  Â  Â  Â  # @st.cache_data # Example of fetching language data (uncomment and implement)
Â  Â  Â  Â  # def get_repo_languages(owner, repo):
Â  Â  Â  Â  # Â  Â  Â lang_url = f"https://api.github.com/repos/{owner}/{repo}/languages"
Â  Â  Â  Â  # Â  Â  Â try:
Â  Â  Â  Â  # Â  Â  Â  Â  Â r = requests.get(lang_url, timeout=5)
Â  Â  Â  Â  # Â  Â  Â  Â  Â r.raise_for_status()
Â  Â  Â  Â  # Â  Â  Â  Â  Â return r.json() # Returns dict like {"Python": 30203, "JavaScript": 1024}
Â  Â  Â  Â  # Â  Â  Â except Exception as e:
Â  Â  Â  Â  # Â  Â  Â  Â  Â logger.error(f"Failed to fetch language data for {owner}/{repo}: {e}")
Â  Â  Â  Â  # Â  Â  Â  Â  Â return None

Â  Â  Â  Â  # languages = get_repo_languages(gh_owner_insight, gh_repo_insight)
Â  Â  Â  Â  # if languages:
Â  Â  Â  Â  # Â  Â  Â st.write("Language Breakdown (Example - actual chart TBD):")
Â  Â  Â  Â  # Â  Â  Â st.json(languages) # Replace with actual chart later using Plotly, Altair, etc.
Â  Â  Â  Â  # else:
Â  Â  Â  Â  # Â  Â  Â st.warning("Could not fetch language data for the repository to display an example chart.")
Â  Â  else:
Â  Â  Â  Â  st.warning("Load a GitHub repository from the sidebar to see potential insights.")


with tab_metrics:
Â  Â  st.header("ğŸ“ˆ System Metrics")
Â  Â  if st.button("ğŸ“ˆ Fetch System Metrics", key="fetch_metrics_tab_btn_val", use_container_width=True):
Â  Â  Â  Â  logger.info("Fetch System Metrics button clicked.")
Â  Â  Â  Â  # Call your backend API to fetch system metrics
Â  Â  Â  Â  response = make_api_request("GET", METRICS_URL, operation_name="Fetch Metrics")
Â  Â  Â  Â  if response and not response.get("error"):
Â  Â  Â  Â  Â  Â  st.session_state.metrics_data = response # Store metrics data
Â  Â  Â  Â  Â  Â  st.success("System metrics refreshed.")
Â  Â  Â  Â  Â  Â  logger.info("Successfully fetched system metrics.")
Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to display new data
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.session_state.metrics_data = {"error": True, "details": response.get('details', '') if response else 'No response'} # Store error
Â  Â  Â  Â  Â  Â  st.error(f"Metrics fetch failed. {response.get('details', '') if response else 'No response or unexpected format from backend.'}")
Â  Â  Â  Â  Â  Â  logger.error("Failed to fetch system metrics.")

Â  Â  # Display current metrics data from session state
Â  Â  current_metrics = st.session_state.get("metrics_data")
Â  Â  if current_metrics and not current_metrics.get("error"):
Â  Â  Â  Â  st.markdown("### Current System Metrics:")
Â  Â  Â  Â  st.json(current_metrics) # Display the full JSON metrics
Â  Â  elif current_metrics and current_metrics.get("error"):
Â  Â  Â  Â  Â st.error(f"Failed to load metrics: {current_metrics.get('details', 'Unknown error.')}")
Â  Â  else:
Â  Â  Â  Â  st.info("Click 'Fetch System Metrics' to see data.")


# === Voice Agent Section ===
# (Kept as per your last provided file - simpler version without full Gemini bi-directional audio yet)
st.markdown("---")
st.markdown("## ğŸ™ï¸ DebugIQ Voice Agent")
st.caption("Speak your commands to the DebugIQ Agent.")

# Display chat history
for message in st.session_state.chat_history:
Â  Â  with st.chat_message(message["role"]):
Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  # Assuming audio_base64 is present for assistant messages when using voice output
Â  Â  Â  Â  if "audio_base64" in message and message["role"] == "assistant" and st.session_state.get("using_gemini_voice", False):
Â  Â  Â  Â  Â  Â  Â try:
Â  Â  Â  Â  Â  Â  Â  Â  Â # Decode the base64 audio data
Â  Â  Â  Â  Â  Â  Â  Â  Â audio_bytes = base64.b64decode(message["audio_base64"])
Â  Â  Â  Â  Â  Â  Â  Â  Â # Provide the audio data to Streamlit's audio player
Â  Â  Â  Â  Â  Â  Â  Â  Â st.audio(audio_bytes, format="audio/mp3") # Assuming backend provides mp3
Â  Â  Â  Â  Â  Â  Â except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â logger.error(f"Error decoding or playing audio for chat message: {e}")


# WebRTC streamer for capturing audio
# Added ClientSettings import and usage as it was in your original snippet
# You need to import ClientSettings from streamlit_webrtc
from streamlit_webrtc import ClientSettings # Ensure this import is present

try:
Â  Â  ctx = webrtc_streamer(
Â  Â  Â  Â  key=f"voice_agent_stream_{BACKEND_URL}", # Use backend URL in key to differentiate if needed
Â  Â  Â  Â  mode=WebRtcMode.SENDONLY, # We only need to send audio from the browser
Â  Â  Â  Â  client_settings=ClientSettings(
Â  Â  Â  Â  Â  Â  Â rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}, # Example STUN server
Â  Â  Â  Â  Â  Â  Â media_stream_constraints={"audio": True, "video": False} # Only enable audio
Â  Â  Â  Â  ),
Â  Â  Â  Â  # async_processing=True # Consider if audio processing is time-consuming
Â  Â  )
except Exception as e:
Â  Â  st.error(f"Failed to initialize voice agent: {e}")
Â  Â  logger.exception("Error initializing webrtc_streamer for Voice Agent")
Â  Â  ctx = None # Set ctx to None if initialization fails

# Audio processing logic when the streamer is connected and receiving audio
if ctx and ctx.audio_receiver:
Â  Â  voice_status_indicator_main_agent = st.empty() # Placeholder for status messages

Â  Â  # Note: The audio capturing and processing loop using `ctx.audio_receiver.get_frame()`
Â  Â  # and sending to your backend for transcription and command processing is complex
Â  Â  # and requires careful handling of threads, buffering, and API calls.
Â  Â  # The original code snippet for this part was commented out or incomplete.
Â  Â  # Below is a simplified representation based on the structure; you will need
Â  Â  # to implement the actual audio capture, buffering, and sending logic here.

Â  Â  # This part of the logic needs to run in a way that doesn't block the Streamlit rerun.
Â  Â  # Using a separate thread for audio processing and API calls is common in Streamlit.

Â  Â  voice_status_indicator_main_agent.info("Voice agent initialized. Speak now.")

Â  Â  # --- Placeholder for Audio Processing and API Call Logic ---
Â  Â  # You need to implement the loop that reads audio frames from ctx.audio_receiver,
Â  Â  # buffers them, detects speech (optional but recommended), sends to VOICE_TRANSCRIBE_URL,
Â  Â  # gets the transcription, sends commands to COMMAND_URL or GEMINI_CHAT_URL,
Â  Â  # and updates st.session_state.chat_history.
Â  Â  # This often involves a dedicated thread started when ctx.state.playing becomes True.

Â  Â  # Example (conceptual) - Requires a separate thread implementation:
Â  Â  # if ctx.state.playing:
Â  Â  # Â  Â  Â if "audio_processing_thread" not in st.session_state:
Â  Â  # Â  Â  Â  Â  Â st.session_state.audio_processing_thread = start_audio_processing_thread(
Â  Â  # Â  Â  Â  Â  Â  Â  Â ctx.audio_receiver,
Â  Â  # Â  Â  Â  Â  Â  Â  Â VOICE_TRANSCRIBE_URL,
Â  Â  # Â  Â  Â  Â  Â  Â  Â COMMAND_URL, # Or GEMINI_CHAT_URL
Â  Â  # Â  Â  Â  Â  Â  Â  Â st.session_state
Â  Â  # Â  Â  Â )
Â  Â  # Â  Â  Â # Thread would handle reading frames, buffering, sending to API, updating chat_history


Â  Â  # To display real-time transcription feedback or status, you might need
Â  Â  # mechanisms for the processing thread to communicate back to the main Streamlit thread.
Â  Â  # This is an advanced Streamlit pattern.

Â  Â  # --- End Placeholder ---

elif ctx and ctx.state.state == "READY":
Â  Â  Â voice_status_indicator_main_agent = st.empty()
Â  Â  Â voice_status_indicator_main_agent.info("Voice agent ready. Click Start.")
elif ctx:
Â  Â  Â voice_status_indicator_main_agent = st.empty()
Â  Â  Â voice_status_indicator_main_agent.info(f"Voice agent state: {ctx.state.state}")
else:
Â  Â  Â # Error message already shown in the try...except block
Â  Â  Â pass

logger.info("--- Script Rerun End ---")
